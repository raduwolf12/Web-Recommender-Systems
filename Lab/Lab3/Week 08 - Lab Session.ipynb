{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e29377",
   "metadata": {},
   "source": [
    "# Advanced Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generated in Session 1 or the provided data splits (see Absalon, W7 Lab)\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_pickle(\"train_dataframe.pkl\")\n",
    "df_test = pd.read_pickle(\"test_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee73231",
   "metadata": {},
   "source": [
    "## Exercise 1 {-}\n",
    "\n",
    "For this exercise, you can use the Python library Scikit-Surprise. Please find the documentation here: https://surprise.readthedocs.io/en/stable/getting_started.html.\n",
    "\n",
    "Define an SVD model with user and item biases that uses Stochastic Gradient Descend (SGD) to estimate the low-rank matrix based on only observed ratings.\n",
    "\n",
    "Fit the model on the full training set with $30$ latent factors and $100$ epochs. Keep Scikit-Surprise's default setting for all other parameters, but set the random state to $0$ for comparable results.\n",
    "\n",
    "Use the model to predict the unobserved ratings for the users in the training set (round your prediction to the third decimal point). How many predictions are there and what is the average of all the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f14ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "\n",
    "# Convert train data format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_matrix = Dataset.load_from_df(df_train[['reviewerID', 'asin', 'overall']], reader)\n",
    "\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "#Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d0c69",
   "metadata": {},
   "source": [
    "## Exercise 2 {-}\n",
    "\n",
    "We will implement the Neural Matrix Factorization model using the Python library RecBole.\n",
    "Please find the documentation here: https://recbole.io/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1879e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and run the following line if you need to install RecBole\n",
    "#!pip install recbole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and run the following line if you need to install ray. This is needed when calling run_recbole\n",
    "#!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from recbole.quick_start import run_recbole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49bbb1",
   "metadata": {},
   "source": [
    "### Exercise 2.1 {-}\n",
    "\n",
    "Convert the dataset to the format which can be read by RecBole.\n",
    "\n",
    "More information regarding the input data format can be found here:\n",
    "https://recbole.io/docs/user_guide/usage/running_new_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfcbae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are creating a dictionary that maps the column names in our dataset to the column names required by RecBole\n",
    "\n",
    "#Fill this dictionary with keys that are column names in our dataset that correspond to user_id, item_id, rating, and timestamp\n",
    "#Fill the values of the dictionary according to the given documentation\n",
    "\n",
    "col_name_dict = {\n",
    "                #write your code here.\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "570e0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method converts a dataframe to a .inter file, and saves it in the folder \"data\" under the name 'file_name'\n",
    "def convert_df_to_inter(df:pd.DataFrame, col_name_dict:dict, file_name:str):\n",
    "    inter = df.copy()\n",
    "    selected_cols = col_name_dict.keys()\n",
    "    inter = inter[selected_cols]\n",
    "    #write your code to rename the columns in inter using col_name_dict\n",
    "\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "    inter.to_csv(\"data/\"+file_name, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23315418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an extra, empty dataframe with the same column names in the keys of col_name_dict\n",
    "#we will use this as a dummy validation file\n",
    "df_extra = #write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a639e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_df_to_inter(df_train, col_name_dict, \"data.train.inter\")\n",
    "convert_df_to_inter(df_test, col_name_dict, \"data.test.inter\")\n",
    "convert_df_to_inter(df_extra, col_name_dict, \"data.extra.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe5a09",
   "metadata": {},
   "source": [
    "### Exercise 2.2 {-}\n",
    "Train the Neural Matrix Factorization model on the whole training dataset for $100$ epochs.\n",
    "\n",
    "Evaluate the model on the test set, based on HR, MRR, Precision, MAP, and Recall at $k \\in \\{5, 10, 20\\}$ respectively and round the scores up to 4 decimal places (It is fine if you have different results in the third decimal point).\n",
    "Keep the rest of the default settings of RecBole the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c443c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_recbole(\n",
    "                    model=#write your code here,\n",
    "                    dataset=\"data\",\n",
    "                    config_dict={\n",
    "                        \"data_path\":\"./\",\n",
    "                        \"benchmark_filename\": ['train', 'extra', 'test'],\n",
    "                        #complete the rest of the dictionary\n",
    "                    })\n",
    "\n",
    "#print the results here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb09d0",
   "metadata": {},
   "source": [
    "## Exercise 3 {-}\n",
    "\n",
    "Let's create a graph-based recommender system, defining neighbourhoods with random walks. Build a bipartite graph (i.e., edges only between users and items) where nodes are users and items; a **bidirectional** edge $(u,i)$ exists in the graph if user $u$ has rated item $i$ with a score $>3$. \n",
    "\n",
    "Implement the Page Rank algorithm to find the top-10 recommended items for user `ARARUVZ8RUF5T`. You can use the `pagerank` method from the library `networkx`. Assume a damping factor of $0.85$ and leave the rest of parameters by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad322ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "\n",
    "# Prepare the data\n",
    "def convert_data(df):\n",
    "    df_convert = #get the rows in the df where the rating is >3\n",
    "    df_convert = df_convert[[\"asin\",\"reviewerID\"]]\n",
    "    df_convert_arr = df_convert.values\n",
    "    return df_convert_arr\n",
    "\n",
    "train_df = convert_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975926df",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Hyper Parameters '''\n",
    "def parameter_dict_from_vector(vector):\n",
    "    return {\n",
    "        \"W_USER_ITEM\" : vector[0],\n",
    "        \"W_USER_ITEM_BACK\" : vector[1]\n",
    "        }\n",
    "\n",
    "''' Building Graph '''\n",
    "class InteractionGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.MultiDiGraph()\n",
    "        \n",
    "    def add_nodes_from_edge_array(self, edge_array, type_1, type_2):\n",
    "        nodes = [(x[0], {'type': type_1}) for x in edge_array] \\\n",
    "        + [(x[1], {'type': type_2}) for x in edge_array]\n",
    "        self.graph.add_nodes_from(nodes)\n",
    "\n",
    "    def add_edges_from_array(self, array, weight_front=1.0, weight_back=1.0):\n",
    "        forward_edges = [(x[0], x[1], weight_front) for x in array]\n",
    "        back_edges = [(x[1], x[0], weight_back) for x in array]\n",
    "        self.graph.add_weighted_edges_from(forward_edges)\n",
    "        self.graph.add_weighted_edges_from(back_edges)\n",
    "\n",
    "def build_graph(parameter_dictionary, user_item_array):\n",
    "    multigraph = InteractionGraph()\n",
    "    multigraph.add_nodes_from_edge_array(user_item_array, 'item', 'user')\n",
    "    multigraph.add_edges_from_array(user_item_array, \n",
    "                                    parameter_dictionary[\"W_USER_ITEM\"], \n",
    "                                    parameter_dictionary[\"W_USER_ITEM_BACK\"])\n",
    "    return multigraph\n",
    "\n",
    "class RecommendationEngine:\n",
    "    def __init__(self, multigraph, damping_factor = 0.3):\n",
    "        self.graph = nx.DiGraph()\n",
    "        for u,v,d in multigraph.graph.edges(data=True):\n",
    "            w = d['weight']\n",
    "            if self.graph.has_edge(u,v):\n",
    "                self.graph[u][v]['weight'] += w\n",
    "            else:\n",
    "                self.graph.add_edge(u,v,weight=w)\n",
    "        self.nodes = list(self.graph.nodes)\n",
    "        self.damping_factor = damping_factor\n",
    "        \n",
    "        #this part keeps track of items that have been rated by each user in the training set\n",
    "        self.user_item_dict = {}\n",
    "        for n in multigraph.graph.nodes.data():\n",
    "            if n[1]['type'] == 'user':\n",
    "                 self.user_item_dict[n[0]] = set()\n",
    "        for e in multigraph.graph.edges:\n",
    "            if e[0] in self.user_item_dict:\n",
    "                 self.user_item_dict[e[0]].add(e[1])\n",
    "\n",
    "    def generate_pr(self, damping_factor):\n",
    "        pr = #use the pagerank method here\n",
    "        pr_sorted = dict(\n",
    "            #sort pr by descending probability values\n",
    "            )\n",
    "        pr_list = [(k, v) for k, v in pr_sorted.items()]\n",
    "        return pr_list\n",
    "    \n",
    "    def generate_recommendations(self, user):\n",
    "        pr_list = self.generate_pr(self.damping_factor)\n",
    "        result = #for each user, remove items in their recommendation list that they have rated in the training set\n",
    "        #hint: you can use user_item_dict for this\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = #fill this\n",
    "\n",
    "# Build the graph\n",
    "graph = build_graph(parameter_dict_from_vector(np.ones(2)), train_df)\n",
    "# Build the recommender system with page rank\n",
    "recommender = RecommendationEngine(graph, damping_factor)\n",
    "\n",
    "# Get top-K recommendations for the given user \n",
    "user_id = \"ARARUVZ8RUF5T\"\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd76a50",
   "metadata": {},
   "source": [
    "Credits: the provided codes in Exercise 3 are modified from\n",
    "https://arxiv.org/pdf/2301.11009.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa7a60bf0b6c750e372a93aa4197b09b5cd5d62fa9d3d2ccbfb64a895ae267e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
