{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d23701c",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcface96",
   "metadata": {},
   "source": [
    "Based on the same dataset used on previous weeks, let us evaluate the Collaborative Filtering (CF) model implemented last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160a7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits from Week 6, the files are also uploaded in Absalon\n",
    "import pandas as pd \n",
    "train_df = pd.read_pickle(\"train_dataframe.pkl\") \n",
    "test_df = pd.read_pickle(\"test_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5b50d",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Based on the user-based neighborhood model that was created last week, let's make a general system that can be used to generate recommendations for all users and items. The system would take into account the mean rating of each user. We can use Scikit-Surprise for this.\n",
    "https://surprise.readthedocs.io/en/stable/index.html\n",
    "\n",
    "Use cosine as similarity measure and try to vary the (maximum) number of neighbors to take into account when predicting ratings. Set the random state to $0$ for comparable results. Keep Scikit-Surprise's default settings for all other parameters. \n",
    "\n",
    "Is it better to use $1$ or $10$ neighbors? You should determine this based on the Root Mean Square Error (RMSE) over 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554581e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following line if you need to install scikit-surprise\n",
    "# !pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8916aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert train data format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_matrix = Dataset.load_from_df(train_df[['reviewerID', 'asin', 'overall']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6aa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fix the random seed\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "# 3. Define a cross-validation iterator\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "rmse_result = dict()\n",
    "\n",
    "list_neighbour = [1, 10]\n",
    "for neighbour in list_neighbour:\n",
    "    algo = KNNWithMeans(k=neighbour,\n",
    "                        sim_options={\"name\":\"cosine\",\"user_based\":True},\n",
    "                        verbose=False,\n",
    "                        random_state=0)\n",
    "    rmse_result[neighbour] = {}\n",
    "    \n",
    "    fold = 0\n",
    "    for trainset, testset in kf.split(training_matrix):\n",
    "\n",
    "        # train and test algorithm.\n",
    "        algo.fit(trainset)\n",
    "        \n",
    "        predictions_KNN = []\n",
    "        for uid, iid, r_ui in testset:\n",
    "            single_prediction = algo.predict(uid=uid, iid=iid, r_ui=r_ui)\n",
    "            predictions_KNN.append(single_prediction)\n",
    "        df_pred_KNN = pd.DataFrame(predictions_KNN)\n",
    "\n",
    "        rmse_result[neighbour][fold] = mse(df_pred_KNN['est'], df_pred_KNN['r_ui'])# Write your code here\n",
    "\n",
    "        fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbff5869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neighbors with lowest validation RMSE:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "best_neighbor = min(rmse_result, key=lambda x: np.sqrt(list(rmse_result[x].values())).mean())\n",
    "\n",
    "print('Number of neighbors with lowest validation RMSE:')\n",
    "print(best_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5f8a7",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "### 2.1\n",
    "Fit the neigborhood-based model defined in exercise 1 on the full training set with cosine as similarity measure and either $1$ or $10$ neighbors based on what you found to be better in exercise 1. Keep Scikit-Surprise's default settings for all other parameters, but set the random state to $0$ for comparable results.\n",
    "\n",
    "Use the model to predict the unobserved ratings for the users in the training set. Remove predictions for users that are not in the test set (`test_df`).\n",
    "\n",
    "How many predictions are there and what is the average of all the predictions (rounded to 2 decimal places)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d9e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 3003\n",
      "Average of all predictions: 4.86\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the full training set\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True\n",
    "               }\n",
    "algo = KNNWithMeans(k= 10,\n",
    "                    sim_options=sim_options, \n",
    "                    random_state=0, \n",
    "                    verbose=False)\n",
    "\n",
    "train_data = training_matrix.build_full_trainset()\n",
    "algo.fit(train_data)\n",
    "\n",
    "# Predict unobserved ratings for users in the training set\n",
    "unobserved_ratings = []\n",
    "for uid in test_df['reviewerID'].unique():\n",
    "    for iid in train_df['asin'].unique():\n",
    "        if uid not in test_df['reviewerID'].unique():\n",
    "            continue\n",
    "        if iid not in train_df.loc[train_df['reviewerID']==uid, 'asin'].unique():\n",
    "            continue\n",
    "        unobserved_ratings.append((uid, iid, 0))\n",
    "pred_KNN = algo.test(unobserved_ratings)\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "num_predictions = len(pred_KNN)\n",
    "avg_prediction = round(np.mean([pred_KNN[i].est for i in range(num_predictions)]), 2)\n",
    "print(\"Number of predictions:\", num_predictions)\n",
    "print(\"Average of all predictions:\", avg_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b35f40",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Report the RMSE of the rating prediction of users and items in `test_df` (rounded to 3 decimal places).\n",
    "\n",
    "Note that the documentation https://surprise.readthedocs.io/en/stable/predictions_module.html defines `r_ui` as the true rating of user $u$ for item $i$, but in fact, it is the mean rating of all users over all items. It should not be used for any computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d3e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf3c25",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Define a general method to get the top-k recommendations for each user, based on the rating predictions obtained in Exercise 2.1. Discard those predictions that are below $4.0$.\n",
    "\n",
    "Print the top-k with $k=\\{5, 10, 20\\}$ recommendations for the user with ID `ARARUVZ8RUF5T` and its estimated ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c95e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from surprise.prediction_algorithms.predictions import Prediction\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "def get_top_k(predictions: List[Prediction], \n",
    "              k: int, \n",
    "              threshold: float) -> Dict[str, List]:\n",
    "\n",
    "    topk = defaultdict(list)\n",
    "\n",
    "    for pred in predictions:\n",
    "        if pred.est < threshold:\n",
    "            continue\n",
    "        topk[pred.uid].append((pred.iid, pred.est))\n",
    "\n",
    "    for uid in topk:\n",
    "        topk[uid] = sorted(topk[uid], key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    return topk\n",
    "\n",
    "def print_top_k(user_id: str, topk: Dict[str, List]) -> None:\n",
    "    user_ratings = topk[user_id]\n",
    "    print(f\"TOP-{len(user_ratings)} predictions for user {user_id}: {[item for item in user_ratings]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6a2ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-1 predictions for user ARARUVZ8RUF5T: [('B01E7UKR38', 4.08)]\n",
      "TOP-1 predictions for user ARARUVZ8RUF5T: [('B01E7UKR38', 4.08)]\n",
      "TOP-1 predictions for user ARARUVZ8RUF5T: [('B01E7UKR38', 4.08)]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "user_id = 'ARARUVZ8RUF5T'\n",
    "for k in [5,10,20]:\n",
    "    topk = get_top_k(pred_KNN, k, threshold=4.0)\n",
    "    print_top_k(user_id, topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325a14",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Report Precision@k (P@k), MAP@k and the MRR@k with $k=\\{5, 10, 20\\}$ averaged across users for the CF model. Round the scores to 3 decimal places.\n",
    "\n",
    "When computing precision, we consider as relevant items those with an observed rating $\\geq 4.0$ (i.e., those items from the test set with a rating $\\geq 4.0$). Reflect on the differences obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d813afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def precision_at_k(predictions: List[Prediction], \n",
    "                   k: int, \n",
    "                   threshold: float) -> Dict[str, float]:\n",
    "    \"\"\"Compute precision at k for each user\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "        threshold(float): Threshold for discarding predictions with too low ratings\n",
    "    Returns:\n",
    "        A dict where keys are user ids (str)\n",
    "        and values are the P@k (float) for each of them\n",
    "    \"\"\"\n",
    "\n",
    "    precisions = defaultdict(float)\n",
    "    \n",
    "    # First map the predictions to each user.\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    return precisions\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(predictions: List[Prediction], \n",
    "                           df_test: pd.DataFrame,\n",
    "                           k: int, \n",
    "                           threshold: float) -> float:\n",
    "    \"\"\"Compute the mean average precision \n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "        threshold(float): Threshold for discarding predictions with too low ratings\n",
    "    Returns:\n",
    "        The MAP@k (float)\n",
    "    \"\"\"\n",
    "\n",
    "    average_precision_users = []\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    mapk = np.mean(average_precision_users)\n",
    "    return mapk\n",
    "    \n",
    "\n",
    "def mean_reciprocal_rank(predictions: List[Prediction], \n",
    "                         df_test: pd.DataFrame, \n",
    "                         k) -> float:\n",
    "    \"\"\"Compute the mean reciprocal rank \n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        The MRR@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    reciprocal_rank = []\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    mean_rr = np.mean(reciprocal_rank)\n",
    "    return mean_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9711402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- NB BASED --------\n",
    "print(\"Metrics for Neighborhood based CF:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=5, threshold=4.0)\n",
    "print(\"Averaged P@5: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=5, threshold=4.0)\n",
    "print(\"MAP@5: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=5)\n",
    "print(\"MRR@5: {:.3f}\".format(mrr_nb))\n",
    "\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=10, threshold=4.0)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=10, threshold=4.0)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=20, threshold=4.0)\n",
    "print(\"Averaged P@20: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=20, threshold=4.0)\n",
    "print(\"MAP@20: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=20)\n",
    "print(\"MRR@20: {:.3f}\".format(mrr_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c50b",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Based on the top-5, top-10 and top-20 predictions from Exercise 3, compute the system’s hit rate averaged over the total number of users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba976c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(top_k: Dict[str, List[str]],\n",
    "             test_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute the hit rate\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n (output of get_top_k())\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "    Returns:\n",
    "        The average hit rate\n",
    "    \"\"\"\n",
    "    hits_rate = 0\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    return hits_rate\n",
    "\n",
    "print(\"Hit Rate for Neighborhood based CF:\")\n",
    "print(\"Hit Rate (top-5): {:.3f}\".format(hit_rate( #Complete )))\n",
    "print(\"Hit Rate (top-10): {:.3f}\".format(hit_rate( #Complete )))\n",
    "print(\"Hit Rate (top-20): {:.3f}\".format(hit_rate( #Complete )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa7a60bf0b6c750e372a93aa4197b09b5cd5d62fa9d3d2ccbfb64a895ae267e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
