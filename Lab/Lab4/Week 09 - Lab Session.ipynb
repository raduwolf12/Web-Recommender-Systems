{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uZ3Rb6JRfF3"
   },
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generated in Session 1 or the provided data splits (see Absalon, W7 Lab)\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_pickle(\"train_dataframe.pkl\")\n",
    "df_test = pd.read_pickle(\"test_dataframe.pkl\")\n",
    "\n",
    "# In this session, we will also need to load the metadata file (see Absalon, W9 Lab)\n",
    "meta_file = 'meta_All_Beauty.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XIYb2Pxi6_Ie"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Load the [metadata file](https://nijianmo.github.io/amazon/index.html) and discard any item that was not rated by our subset of users (not in training or test sets). Apply preprocessing in this order: lowercasing, stemming, tokenizing, and stopwords removal to clean up the text from the `title`. Report the vocabulary size before and after the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKbUQSlc65kO",
    "outputId": "f43a9bb4-8d8f-4653-df9b-613fa23e1bb5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "# Load the metadata (items)\n",
    "metadata = []\n",
    "with open(meta_file, 'r') as f:\n",
    "    metadata = []\n",
    "    for line in f:\n",
    "        metadata.append(json.loads(line))\n",
    "\n",
    "metadata = pd.DataFrame(metadata)\n",
    "\n",
    "\n",
    "\n",
    "# Discard items that weren't rated by our subset of users\n",
    "# Create a set of user IDs from the training and test sets\n",
    "user_set = set(df_train['reviewerID']).union(set(df_test['reviewerID']))\n",
    "\n",
    "# Filter the metadata to only include items that were rated by our subset of users\n",
    "metadata = metadata[metadata['asin'].isin(set(df_train['asin']).union(set(df_test['asin'])))]\n",
    "metadata = metadata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items: 88\n"
     ]
    }
   ],
   "source": [
    "total_items = len(metadata)\n",
    "print(f'Total number of items: {total_items}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   category tech1                                        description fit  \\\n0        []        [INDICATIONS: Aqua Velva Cooling After Shave E...       \n1        []        [<P><STRONG>Restores Moisture to Dehydrated Ha...       \n2        []        [A richly pigmented, micronized powder formula...       \n3        []        [Avalon Organics Wrinkle Therapy Cleansing Mil...       \n4        []        [INDICATIONS: Aqua Velva Cooling After Shave E...       \n..      ...   ...                                                ...  ..   \n83       []                             [new authentic discontinued]       \n84       []        [Launched by the design house of dolce and gab...       \n85       []        [Colgate Kids Maximum Cavity Protection Pump T...       \n86       []                                                       []       \n87       []        [the pleasure is all mine. loose the formaliti...       \n\n                                                title  \\\n0   Aqua Velva After Shave, Classic Ice Blue, 7 Ounce   \n1       Citre Shine Moisture Burst Shampoo - 16 fl oz   \n2                               NARS Blush, Taj Mahal   \n3   Avalon Organics Wrinkle Therapy CoQ10 Cleansin...   \n4   Aqua Velva After Shave, Classic Ice Blue, 7 Ounce   \n..                                                ...   \n83         Ultimate Body Lotion By Michael Kors 3.4oz   \n84     Dolce &amp; Gabbana Compact Parfum, 0.05 Ounce   \n85  Colgate Kids Maximum Cavity Protection Pump To...   \n86  Bali Secrets Natural Deodorant - Organic &amp;...   \n87                      essie Gel Couture Nail Polish   \n\n                                             also_buy tech2            brand  \\\n0   [B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...             Aqua Velva   \n1   [B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...            Citre Shine   \n2                                                  []                   NARS   \n3   [B0014407HC, B001ECQ41M, B00503OFIU, B00015XAQ...                 Avalon   \n4   [B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...             Aqua Velva   \n..                                                ...   ...              ...   \n83                                       [B00R1QRWLG]           Michael Kors   \n84                                                 []        Dolce & Gabbana   \n85  [B07G35BLNY, B01MR4VL2E, B003XDX68E, B002VA4FY...                Colgate   \n86  [B07DNGSBSC, B00EOB0042, B077F4NB55, B07GXZYBX...                          \n87  [B01E7UKPWQ, B01E7UKTAE, B01E7UKS7S, B071RKDH9...                          \n\n                                              feature  \\\n0                                                  []   \n1                                                  []   \n2                                                  []   \n3                                                  []   \n4                                                  []   \n..                                                ...   \n83                                                 []   \n84                                                 []   \n85  [<span class=\"a-size-base a-color-secondary\">\\...   \n86                                                 []   \n87                                                 []   \n\n                                       rank  \\\n0        65,003 in Beauty & Personal Care (   \n1     1,693,702 in Beauty & Personal Care (   \n2       505,302 in Beauty & Personal Care (   \n3   141,988 in Beauty &amp; Personal Care (   \n4        65,003 in Beauty & Personal Care (   \n..                                      ...   \n83      736,956 in Beauty & Personal Care (   \n84      863,502 in Beauty & Personal Care (   \n85      307,236 in Beauty & Personal Care (   \n86      152,867 in Beauty & Personal Care (   \n87      306,101 in Beauty & Personal Care (   \n\n                                            also_view  \\\n0   [B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...   \n1                                                  []   \n2   [B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...   \n3   [B077ZG4C3L, B07DW6ZLFS, B00503OFIU, B07DVZMGL...   \n4   [B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...   \n..                                                ...   \n83                           [B016AGM1MW, B019AWE8TW]   \n84                                                 []   \n85                                                 []   \n86                                                 []   \n87                                                 []   \n\n                                              details    main_cat  \\\n0   {'\n    Product Dimensions: \n    ': '3 x 4 x 5 ...  All Beauty   \n1   {'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...  All Beauty   \n2   {'\n    Item Weight: \n    ': '0.16 ounces', 'Sh...  All Beauty   \n3   {'\n    Product Dimensions: \n    ': '2.5 x 1.4 ...  All Beauty   \n4   {'\n    Product Dimensions: \n    ': '3 x 4 x 5 ...  All Beauty   \n..                                                ...         ...   \n83   {'ASIN: ': 'B019LAI4HU', 'UPC:': '682821146329'}  All Beauty   \n84  {'\n    Product Dimensions: \n    ': '3.5 x 1.2 ...  All Beauty   \n85  {'\n    Product Dimensions: \n    ': '2 x 0.9 x ...  All Beauty   \n86  {'\n    Product Dimensions: \n    ': '4.3 x 1.8 ...  All Beauty   \n87  {'\n    Product Dimensions: \n    ': '1.5 x 1.5 ...  All Beauty   \n\n   similar_item date   price        asin  \\\n0                             B0000530HU   \n1                     $23.00  B00006L9LC   \n2                     $34.50  B00021DJ32   \n3                      $8.27  B0002JHI1I   \n4                             B0000530HU   \n..          ...  ...     ...         ...   \n83                            B019LAI4HU   \n84                            B019V2KYZS   \n85                            B01BNEYGQU   \n86                            B01DKQAXC0   \n87                    $11.25  B01E7UKR38   \n\n                                             imageURL  \\\n0   [https://images-na.ssl-images-amazon.com/image...   \n1                                                  []   \n2   [https://images-na.ssl-images-amazon.com/image...   \n3   [https://images-na.ssl-images-amazon.com/image...   \n4   [https://images-na.ssl-images-amazon.com/image...   \n..                                                ...   \n83                                                 []   \n84  [https://images-na.ssl-images-amazon.com/image...   \n85  [https://images-na.ssl-images-amazon.com/image...   \n86  [https://images-na.ssl-images-amazon.com/image...   \n87  [https://images-na.ssl-images-amazon.com/image...   \n\n                                      imageURLHighRes  \n0   [https://images-na.ssl-images-amazon.com/image...  \n1                                                  []  \n2   [https://images-na.ssl-images-amazon.com/image...  \n3   [https://images-na.ssl-images-amazon.com/image...  \n4   [https://images-na.ssl-images-amazon.com/image...  \n..                                                ...  \n83                                                 []  \n84  [https://images-na.ssl-images-amazon.com/image...  \n85  [https://images-na.ssl-images-amazon.com/image...  \n86  [https://images-na.ssl-images-amazon.com/image...  \n87  [https://images-na.ssl-images-amazon.com/image...  \n\n[88 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>tech1</th>\n      <th>description</th>\n      <th>fit</th>\n      <th>title</th>\n      <th>also_buy</th>\n      <th>tech2</th>\n      <th>brand</th>\n      <th>feature</th>\n      <th>rank</th>\n      <th>also_view</th>\n      <th>details</th>\n      <th>main_cat</th>\n      <th>similar_item</th>\n      <th>date</th>\n      <th>price</th>\n      <th>asin</th>\n      <th>imageURL</th>\n      <th>imageURLHighRes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[]</td>\n      <td></td>\n      <td>[INDICATIONS: Aqua Velva Cooling After Shave E...</td>\n      <td></td>\n      <td>Aqua Velva After Shave, Classic Ice Blue, 7 Ounce</td>\n      <td>[B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...</td>\n      <td></td>\n      <td>Aqua Velva</td>\n      <td>[]</td>\n      <td>65,003 in Beauty &amp; Personal Care (</td>\n      <td>[B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...</td>\n      <td>{'\n    Product Dimensions: \n    ': '3 x 4 x 5 ...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>B0000530HU</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[]</td>\n      <td></td>\n      <td>[&lt;P&gt;&lt;STRONG&gt;Restores Moisture to Dehydrated Ha...</td>\n      <td></td>\n      <td>Citre Shine Moisture Burst Shampoo - 16 fl oz</td>\n      <td>[B07CSVCGZV, B07KMGC13Z, B0793XJ4WW, B01N7U1HB...</td>\n      <td></td>\n      <td>Citre Shine</td>\n      <td>[]</td>\n      <td>1,693,702 in Beauty &amp; Personal Care (</td>\n      <td>[]</td>\n      <td>{'ASIN: ': 'B00006L9LC', 'UPC:': '795827187965...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td>$23.00</td>\n      <td>B00006L9LC</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[]</td>\n      <td></td>\n      <td>[A richly pigmented, micronized powder formula...</td>\n      <td></td>\n      <td>NARS Blush, Taj Mahal</td>\n      <td>[]</td>\n      <td></td>\n      <td>NARS</td>\n      <td>[]</td>\n      <td>505,302 in Beauty &amp; Personal Care (</td>\n      <td>[B07FVJJ39R, B07JBQZDKB, B07HKVJC7G, B010VWL4E...</td>\n      <td>{'\n    Item Weight: \n    ': '0.16 ounces', 'Sh...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td>$34.50</td>\n      <td>B00021DJ32</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[]</td>\n      <td></td>\n      <td>[Avalon Organics Wrinkle Therapy Cleansing Mil...</td>\n      <td></td>\n      <td>Avalon Organics Wrinkle Therapy CoQ10 Cleansin...</td>\n      <td>[B0014407HC, B001ECQ41M, B00503OFIU, B00015XAQ...</td>\n      <td></td>\n      <td>Avalon</td>\n      <td>[]</td>\n      <td>141,988 in Beauty &amp;amp; Personal Care (</td>\n      <td>[B077ZG4C3L, B07DW6ZLFS, B00503OFIU, B07DVZMGL...</td>\n      <td>{'\n    Product Dimensions: \n    ': '2.5 x 1.4 ...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td>$8.27</td>\n      <td>B0002JHI1I</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[]</td>\n      <td></td>\n      <td>[INDICATIONS: Aqua Velva Cooling After Shave E...</td>\n      <td></td>\n      <td>Aqua Velva After Shave, Classic Ice Blue, 7 Ounce</td>\n      <td>[B00J232PCM, B0010V5MKG, B000052Y68, B00KOAIU7...</td>\n      <td></td>\n      <td>Aqua Velva</td>\n      <td>[]</td>\n      <td>65,003 in Beauty &amp; Personal Care (</td>\n      <td>[B01I9TIY1U, B07L1PZCS7, B01N12C89Y, B01I9TINT...</td>\n      <td>{'\n    Product Dimensions: \n    ': '3 x 4 x 5 ...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>B0000530HU</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>[]</td>\n      <td></td>\n      <td>[new authentic discontinued]</td>\n      <td></td>\n      <td>Ultimate Body Lotion By Michael Kors 3.4oz</td>\n      <td>[B00R1QRWLG]</td>\n      <td></td>\n      <td>Michael Kors</td>\n      <td>[]</td>\n      <td>736,956 in Beauty &amp; Personal Care (</td>\n      <td>[B016AGM1MW, B019AWE8TW]</td>\n      <td>{'ASIN: ': 'B019LAI4HU', 'UPC:': '682821146329'}</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>B019LAI4HU</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>[]</td>\n      <td></td>\n      <td>[Launched by the design house of dolce and gab...</td>\n      <td></td>\n      <td>Dolce &amp;amp; Gabbana Compact Parfum, 0.05 Ounce</td>\n      <td>[]</td>\n      <td></td>\n      <td>Dolce &amp; Gabbana</td>\n      <td>[]</td>\n      <td>863,502 in Beauty &amp; Personal Care (</td>\n      <td>[]</td>\n      <td>{'\n    Product Dimensions: \n    ': '3.5 x 1.2 ...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>B019V2KYZS</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>[]</td>\n      <td></td>\n      <td>[Colgate Kids Maximum Cavity Protection Pump T...</td>\n      <td></td>\n      <td>Colgate Kids Maximum Cavity Protection Pump To...</td>\n      <td>[B07G35BLNY, B01MR4VL2E, B003XDX68E, B002VA4FY...</td>\n      <td></td>\n      <td>Colgate</td>\n      <td>[&lt;span class=\"a-size-base a-color-secondary\"&gt;\\...</td>\n      <td>307,236 in Beauty &amp; Personal Care (</td>\n      <td>[]</td>\n      <td>{'\n    Product Dimensions: \n    ': '2 x 0.9 x ...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>B01BNEYGQU</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>[]</td>\n      <td></td>\n      <td>[]</td>\n      <td></td>\n      <td>Bali Secrets Natural Deodorant - Organic &amp;amp;...</td>\n      <td>[B07DNGSBSC, B00EOB0042, B077F4NB55, B07GXZYBX...</td>\n      <td></td>\n      <td></td>\n      <td>[]</td>\n      <td>152,867 in Beauty &amp; Personal Care (</td>\n      <td>[]</td>\n      <td>{'\n    Product Dimensions: \n    ': '4.3 x 1.8 ...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>B01DKQAXC0</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>[]</td>\n      <td></td>\n      <td>[the pleasure is all mine. loose the formaliti...</td>\n      <td></td>\n      <td>essie Gel Couture Nail Polish</td>\n      <td>[B01E7UKPWQ, B01E7UKTAE, B01E7UKS7S, B071RKDH9...</td>\n      <td></td>\n      <td></td>\n      <td>[]</td>\n      <td>306,101 in Beauty &amp; Personal Care (</td>\n      <td>[]</td>\n      <td>{'\n    Product Dimensions: \n    ': '1.5 x 1.5 ...</td>\n      <td>All Beauty</td>\n      <td></td>\n      <td></td>\n      <td>$11.25</td>\n      <td>B01E7UKR38</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n    </tr>\n  </tbody>\n</table>\n<p>88 rows × 19 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uncomment and run this to install nltk\n",
    "# !pip install nltk\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rd0RbH-k9y1H",
    "outputId": "ab4de5c7-9660-47ab-875f-cde5077324f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size before preprocessing: 568\n",
      "Vocabulary size after preprocessing: 484\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "vocab_before = len(set(' '.join(metadata['title']).split()))\n",
    "\n",
    "# # Preprocess the title column\n",
    "# stemmer = PorterStemmer()\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# metadata['title'] = metadata['title'].apply(lambda x: ' '.join([stemmer.stem(w.lower()) for w in nltk.word_tokenize(x) if w.lower() not in stop_words]))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Preprocess the 'title' column\n",
    "metadata['title'] = metadata['title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "vocab_after = len(set(' '.join(metadata['title']).split()))\n",
    "print(f'Vocabulary size before preprocessing: {vocab_before}')\n",
    "print(f'Vocabulary size after preprocessing: {vocab_after}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Blr1jgoHLbFU"
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Representation in vector spaces.\n",
    "\n",
    "## 2.1\n",
    "\n",
    "Represent all the items from Exercise 1 in a TF-IDF space. Interpret the meaning of the TF-IDF matrix dimensions.\n",
    "\n",
    "Tip: You may use the library [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDndolvDLznV",
    "outputId": "35ddbd7f-2bcf-4289-9436-97cbc3ca26bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix dimensions: (88, 475)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, tokenizer=nltk.word_tokenize, stop_words='english',\n",
    "                             strip_accents='unicode', use_idf=True, norm='l2')\n",
    "\n",
    "# Apply the vectorizer to the 'title' column of the metadata DataFrame\n",
    "tfidf_matrix = vectorizer.fit_transform(metadata['title'])\n",
    "\n",
    "# Print the dimensions of the TF-IDF matrix\n",
    "print(f'TF-IDF matrix dimensions: {tfidf_matrix.shape}')\n",
    "\n",
    "# Save results for future use\n",
    "import pickle\n",
    "\n",
    "# Save the TF-IDF matrix to a file\n",
    "with open('tfidf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "# Save the vectorizer object to a file\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-gnTVM0EV_2d"
   },
   "source": [
    "## 2.2\n",
    "\n",
    "Using the TF-IDF representation, compute the cosine similarity between products with asin `B000FI4S1E`, `B000LIBUBY` and `B000W0C07Y`. Take a look at their features to see whether results make sense with their characteristics. Round your final answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zO_OHMY8PWbO",
    "outputId": "8fd3f26c-7adc-4c95-a8ce-febdabe05406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between 'B000FI4S1E' and 'B000LIBUBY' is 0.025\n",
      "The cosine similarity between 'B000FI4S1E' and 'B000W0C07Y' is 0.020\n",
      "The cosine similarity between 'B000LIBUBY' and 'B000W0C07Y' is 0.443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "asin1 = 'B000FI4S1E' \n",
    "asin2 = 'B000LIBUBY'\n",
    "asin3 ='B000W0C07Y'\n",
    "\n",
    "# Get the indices of the products with the given ASINs\n",
    "idx1 = metadata[metadata['asin'] == asin1].index[0]\n",
    "idx2 = metadata[metadata['asin'] == asin2].index[0]\n",
    "idx3 = metadata[metadata['asin'] == asin3].index[0]\n",
    "\n",
    "# Get the corresponding rows from the TF-IDF matrix\n",
    "row1 = tfidf_matrix[idx1]\n",
    "row2 = tfidf_matrix[idx2]\n",
    "row3 = tfidf_matrix[idx3]\n",
    "\n",
    "# Compute the cosine similarity between the rows\n",
    "similarity12 = cosine_similarity(row1.reshape(1, -1), row2.reshape(1, -1))[0][0]\n",
    "similarity13 = cosine_similarity(row1.reshape(1, -1), row3.reshape(1, -1))[0][0]\n",
    "similarity23 = cosine_similarity(row2.reshape(1, -1), row3.reshape(1, -1))[0][0]\n",
    "\n",
    "print(f\"The cosine similarity between '{asin1}' and '{asin2}' is {similarity12:.3f}\")\n",
    "print(f\"The cosine similarity between '{asin1}' and '{asin3}' is {similarity13:.3f}\")\n",
    "print(f\"The cosine similarity between '{asin2}' and '{asin3}' is {similarity23:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0K8jRhWhZQWe"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "Representation in vector spaces with contextual Word Embeddings.\n",
    "\n",
    "## 3.1.\n",
    "\n",
    "Represent all the products from Exercise 1 in a vector space using embeddings from a pre-trained BERT model. The final embedding of a product should be the average of the word embeddings from all the words in the `title`. \n",
    "\n",
    "What is the vocabulary size of the model? What are the dimensions of the last hidden state?\n",
    "\n",
    "Tip: you may install the transformers library and use their pretrained [BERT model uncased](https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: importlib-metadata in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: requests in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from transformers) (22.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.2/199.2 kB 6.1 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.2/153.2 kB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from importlib-metadata->transformers) (3.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Installing collected packages: tokenizers, pyyaml, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.9.0 huggingface-hub-0.13.1 pyyaml-6.0 tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "#Uncomment and run the following line to install the transformers library\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp37-cp37m-win_amd64.whl (2434.1 MB)\n",
      "     ---------------------------------------- 2.4/2.4 GB 790.3 kB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp37-cp37m-win_amd64.whl (4.8 MB)\n",
      "     ---------------------------------------- 4.8/4.8 MB 8.1 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp37-cp37m-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: requests in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\programe\\anaconda3\\envs\\web\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.13.1+cu116 torchaudio-0.13.1+cu116 torchvision-0.14.1+cu116\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHIjJ-LbTB3H",
    "outputId": "0bec9e50-22cc-4463-8daf-d3c62b66d0cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c51d3c8f5b0f41a5bb3bbb3f862437ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programe\\Anaconda3\\envs\\web\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Radu\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e405f2c741b74abaae5e2307e5c87d7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57d59996953f4180a1eb5a6306ccffa0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32cd6c1af8c54e3dafc5fc35ccde36d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "113d3251f52f45979c8dd57eb00910f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of the bert-base-uncased model: 30522\n"
     ]
    }
   ],
   "source": [
    "# LOAD TRANSFORMER\n",
    "\"\"\"\n",
    "If you plan on using a pretrained model, it’s important to use the associated \n",
    "pretrained tokenizer: it will split the text you give it in tokens the same way\n",
    "for the pretraining corpus, and it will use the same correspondence\n",
    "token to index (that we usually call a vocab) as during pretraining.\n",
    "\"\"\"\n",
    "\n",
    "# % pip install transformers\n",
    "import torch\n",
    "import transformers\n",
    "assert transformers.__version__ > '4.0.0'\n",
    "\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "# set-up environment\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "modelname = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
    "model = BertModel.from_pretrained(modelname).to(DEVICE)\n",
    "\n",
    "# Print out the vocabulary size\n",
    "# YOUR CODE HERE\n",
    "print(f\"Vocabulary size of the {modelname} model: {len(tokenizer.get_vocab())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_Symyv5U07x",
    "outputId": "580c6162-03ef-4041-d4a9-c2552181d4d2"
   },
   "outputs": [],
   "source": [
    "# Represent products in a vector space\n",
    "\"\"\"\n",
    "When using pre-trained models, it is always advised to feed it data similar to what it was trained with. \n",
    "Basically, it doesn't hurt to keep all the words in.\n",
    "However, the effect (or the lack of it) will vary based on corpus and task. \n",
    "Decision here: keep them all since pretraining was done that way.\n",
    "\"\"\"\n",
    "\n",
    "def batch_encoding(sentences):\n",
    "    # Since we're using padding, we need to provide the attention masks to our\n",
    "    # model. Otherwise it doesn't know which tokens it should not attend to. \n",
    "    inputs =  tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    print(inputs) # Look at the padding and attention_mask\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    last_hidden_states =  outputs.hidden_states[-1]\n",
    "    # attention_mask = inputs['attention_mask']\n",
    "    # last_hidden_states[attention_mask == 0] = 0\n",
    "\n",
    "    return inputs, last_hidden_states\n",
    "  \n",
    "encoded_inputs, title_last_hidden_states = batch_encoding(\n",
    "                                                            # <YOUR CODE HERE>\n",
    "                                                            )\n",
    "# Note that the control token [CLS] has been added at the beginning of each sentence,\n",
    "# and [SEP] at the end\n",
    "# Let's mask out the padding tokens \n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EwRBr2HP0Zdt"
   },
   "source": [
    "## 3.2.\n",
    "\n",
    "Using the representation obtained from Exercise 3.1., compute the cosine similarity between items with asin `B000FI4S1E`, `B000LIBUBY` and `B000W0C07Y`.\n",
    "Round your final answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaHxSLHqItNs",
    "outputId": "1f964bb4-5c37-4278-cccf-848e7b552183"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "asin1 = 'B000FI4S1E' \n",
    "asin2 = 'B000LIBUBY'\n",
    "asin3 ='B000W0C07Y'\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Session_4-Text-Representation-solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wrs2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa7a60bf0b6c750e372a93aa4197b09b5cd5d62fa9d3d2ccbfb64a895ae267e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
